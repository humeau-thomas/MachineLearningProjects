{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Machine learning, Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we are given the Enron dataset, and the goal is to identify the persons of interest in the Enron scandal. We are provided with financial features, like salary, stock options, etc... and some email features, detailing the exchanges between the collaborators. We first import the datasets. The cleaned datasets are saved as pickle files, and can be accessed directly from [here](#section3). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Importation of the data](#section1) \n",
    "\n",
    "2. [Data Cleaning and Outliers](#section2)\n",
    "\n",
    "3. [Start Classification from cleaned data](#section3)\n",
    "\n",
    "   3.1. [Feature engineering](#section3.1)\n",
    "   \n",
    "   3.2. [Dimensionality reduction](#section3.2)\n",
    "\n",
    "4. [Classification](#section4)\n",
    "\n",
    "    4.1. [Gaussian Naive Bayes](#section4.1)\n",
    "    \n",
    "    4.2. [Logistic Regression](#section4.2)\n",
    "    \n",
    "    4.3. [Decision Tree](#section4.3)\n",
    "    \n",
    "    4.4. [Random Forest](#section4.4)\n",
    "    \n",
    "    4.5. [AdaBoost](#section4.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importation of the data:\n",
    "<a id='section1'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   salary  to_messages  deferral_payments  total_payments  \\\n",
      "ALLEN PHILLIP K  201955.0       2902.0          2869717.0       4484442.0   \n",
      "BADUM JAMES P         NaN          NaN           178980.0        182466.0   \n",
      "\n",
      "                 exercised_stock_options      bonus  restricted_stock  \\\n",
      "ALLEN PHILLIP K                1729541.0  4175000.0          126027.0   \n",
      "BADUM JAMES P                   257817.0        NaN               NaN   \n",
      "\n",
      "                 shared_receipt_with_poi  restricted_stock_deferred  \\\n",
      "ALLEN PHILLIP K                   1407.0                  -126027.0   \n",
      "BADUM JAMES P                        NaN                        NaN   \n",
      "\n",
      "                 total_stock_value           ...            loan_advances  \\\n",
      "ALLEN PHILLIP K          1729541.0           ...                      NaN   \n",
      "BADUM JAMES P             257817.0           ...                      NaN   \n",
      "\n",
      "                 from_messages  other  from_this_person_to_poi    poi  \\\n",
      "ALLEN PHILLIP K         2195.0  152.0                     65.0  False   \n",
      "BADUM JAMES P              NaN    NaN                      NaN  False   \n",
      "\n",
      "                director_fees  deferred_income  long_term_incentive  \\\n",
      "ALLEN PHILLIP K           NaN       -3081055.0             304805.0   \n",
      "BADUM JAMES P             NaN              NaN                  NaN   \n",
      "\n",
      "                           email_address from_poi_to_this_person  \n",
      "ALLEN PHILLIP K  phillip.allen@enron.com                    47.0  \n",
      "BADUM JAMES P                        NaN                     NaN  \n",
      "\n",
      "[2 rows x 21 columns]\n",
      "There are 146 persons in this dataset, with 21 features\n",
      "The features are: ['salary' 'to_messages' 'deferral_payments' 'total_payments'\n",
      " 'exercised_stock_options' 'bonus' 'restricted_stock'\n",
      " 'shared_receipt_with_poi' 'restricted_stock_deferred' 'total_stock_value'\n",
      " 'expenses' 'loan_advances' 'from_messages' 'other'\n",
      " 'from_this_person_to_poi' 'poi' 'director_fees' 'deferred_income'\n",
      " 'long_term_incentive' 'email_address' 'from_poi_to_this_person']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pickle\n",
    "import numpy as np\n",
    "with open(\"../ud120-projects/final_project/final_project_dataset.pkl\", \"r\") as data_file:\n",
    "    data_dict = pickle.load(data_file)\n",
    "import pandas as pd\n",
    "data=pd.DataFrame.from_dict(data_dict, orient='index')\n",
    "data.replace('NaN', np.nan, inplace=True) #replace the NaN values (strings) as np.nan values\n",
    "print(data.head(2))\n",
    "print('There are {} persons in this dataset, with {} features'.format(data.shape[0], data.shape[1]))\n",
    "print('The features are: {}'.format(data.columns.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning and outliers\n",
    "<a id='section2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with some data cleaning, identifying some outliers and absurdities. The first task is to count the number of 'NaN' values for each features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "salary                        51\n",
      "to_messages                   60\n",
      "deferral_payments            107\n",
      "total_payments                21\n",
      "exercised_stock_options       44\n",
      "bonus                         64\n",
      "restricted_stock              36\n",
      "shared_receipt_with_poi       60\n",
      "restricted_stock_deferred    128\n",
      "total_stock_value             20\n",
      "expenses                      51\n",
      "loan_advances                142\n",
      "from_messages                 60\n",
      "other                         53\n",
      "from_this_person_to_poi       60\n",
      "poi                            0\n",
      "director_fees                129\n",
      "deferred_income               97\n",
      "long_term_incentive           80\n",
      "email_address                 35\n",
      "from_poi_to_this_person       60\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "nb_NaN=data.isnull().sum(axis=0)\n",
    "print(nb_NaN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that some of the features have a lot of missing values. Furthermore, some features are irrelevant, like the email address. We drop those features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   salary  to_messages  total_payments  \\\n",
      "ALLEN PHILLIP K  201955.0       2902.0       4484442.0   \n",
      "BADUM JAMES P         NaN          NaN        182466.0   \n",
      "\n",
      "                 exercised_stock_options      bonus  restricted_stock  \\\n",
      "ALLEN PHILLIP K                1729541.0  4175000.0          126027.0   \n",
      "BADUM JAMES P                   257817.0        NaN               NaN   \n",
      "\n",
      "                 shared_receipt_with_poi  total_stock_value  expenses  \\\n",
      "ALLEN PHILLIP K                   1407.0          1729541.0   13868.0   \n",
      "BADUM JAMES P                        NaN           257817.0    3486.0   \n",
      "\n",
      "                 from_messages  other  from_this_person_to_poi    poi  \\\n",
      "ALLEN PHILLIP K         2195.0  152.0                     65.0  False   \n",
      "BADUM JAMES P              NaN    NaN                      NaN  False   \n",
      "\n",
      "                 long_term_incentive  from_poi_to_this_person  \n",
      "ALLEN PHILLIP K             304805.0                     47.0  \n",
      "BADUM JAMES P                    NaN                      NaN  \n"
     ]
    }
   ],
   "source": [
    "irrelevant_features=nb_NaN.index[nb_NaN>80].values\n",
    "irrelevant_features=np.append(irrelevant_features,['email_address'])\n",
    "data.drop(irrelevant_features,axis=1, inplace=True)\n",
    "print(data.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new feature list is given by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['salary' 'to_messages' 'total_payments' 'exercised_stock_options' 'bonus'\n",
      " 'restricted_stock' 'shared_receipt_with_poi' 'total_stock_value'\n",
      " 'expenses' 'from_messages' 'other' 'from_this_person_to_poi' 'poi'\n",
      " 'long_term_incentive' 'from_poi_to_this_person']\n"
     ]
    }
   ],
   "source": [
    "print(data.columns.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we look for outliers in the data. To do this, we select the financial features and print the 5 largest values for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Largest salary:\n",
      "TOTAL                 26704229.0\n",
      "SKILLING JEFFREY K     1111258.0\n",
      "LAY KENNETH L          1072321.0\n",
      "FREVERT MARK A         1060932.0\n",
      "PICKERING MARK R        655037.0\n",
      "Name: salary, dtype: float64\n",
      "\n",
      "Largest total_payments:\n",
      "TOTAL               309886585.0\n",
      "LAY KENNETH L       103559793.0\n",
      "FREVERT MARK A       17252530.0\n",
      "BHATNAGAR SANJAY     15456290.0\n",
      "LAVORATO JOHN J      10425757.0\n",
      "Name: total_payments, dtype: float64\n",
      "\n",
      "Largest exercised_stock_options:\n",
      "TOTAL                 311764000.0\n",
      "LAY KENNETH L          34348384.0\n",
      "HIRKO JOSEPH           30766064.0\n",
      "RICE KENNETH D         19794175.0\n",
      "SKILLING JEFFREY K     19250000.0\n",
      "Name: exercised_stock_options, dtype: float64\n",
      "\n",
      "Largest bonus:\n",
      "TOTAL                 97343619.0\n",
      "LAVORATO JOHN J        8000000.0\n",
      "LAY KENNETH L          7000000.0\n",
      "SKILLING JEFFREY K     5600000.0\n",
      "BELDEN TIMOTHY N       5249999.0\n",
      "Name: bonus, dtype: float64\n",
      "\n",
      "Largest restricted_stock:\n",
      "TOTAL                 130322299.0\n",
      "LAY KENNETH L          14761694.0\n",
      "WHITE JR THOMAS E      13847074.0\n",
      "PAI LOU L               8453763.0\n",
      "SKILLING JEFFREY K      6843672.0\n",
      "Name: restricted_stock, dtype: float64\n",
      "\n",
      "Largest total_stock_value:\n",
      "TOTAL                 434509511.0\n",
      "LAY KENNETH L          49110078.0\n",
      "HIRKO JOSEPH           30766064.0\n",
      "SKILLING JEFFREY K     26093672.0\n",
      "PAI LOU L              23817930.0\n",
      "Name: total_stock_value, dtype: float64\n",
      "\n",
      "Largest expenses:\n",
      "TOTAL                 5235198.0\n",
      "MCCLELLAN GEORGE       228763.0\n",
      "URQUHART JOHN A        228656.0\n",
      "SHANKMAN JEFFREY A     178979.0\n",
      "SHAPIRO RICHARD S      137767.0\n",
      "Name: expenses, dtype: float64\n",
      "\n",
      "Largest other:\n",
      "TOTAL              42667589.0\n",
      "LAY KENNETH L      10359729.0\n",
      "FREVERT MARK A      7427621.0\n",
      "MARTIN AMANDA K     2818454.0\n",
      "BAXTER JOHN C       2660303.0\n",
      "Name: other, dtype: float64\n",
      "\n",
      "Largest long_term_incentive:\n",
      "TOTAL              48521928.0\n",
      "MARTIN AMANDA K     5145434.0\n",
      "LAY KENNETH L       3600000.0\n",
      "ECHOLS JOHN B       2234774.0\n",
      "LAVORATO JOHN J     2035380.0\n",
      "Name: long_term_incentive, dtype: float64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "financial_features=['salary','total_payments' ,'exercised_stock_options', 'bonus',\n",
    " 'restricted_stock' , 'total_stock_value',\n",
    " 'expenses' , 'other' ,\n",
    " 'long_term_incentive' ]\n",
    "for feature in financial_features:\n",
    "    print('Largest {}:'.format(feature))\n",
    "    print(data.nlargest(5,feature)[feature])\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a 'TOTAL' index that is useless here, we drop it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.drop('TOTAL',axis=0,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also check for indexes which have almost no data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOCKHART EUGENE E                14\n",
      "CHAN RONNIE                      13\n",
      "GRAMM WENDY L                    13\n",
      "SAVAGE FRANK                     13\n",
      "BLAKE JR. NORMAN P               12\n",
      "CLINE KENNETH W                  12\n",
      "MENDELSOHN JOHN                  12\n",
      "MEYER JEROME J                   12\n",
      "PEREIRA PAULO V. FERRAZ          12\n",
      "SCRIMSHAW MATTHEW                12\n",
      "THE TRAVEL AGENCY IN THE PARK    12\n",
      "URQUHART JOHN A                  12\n",
      "WAKEHAM JOHN                     12\n",
      "WHALEY DAVID A                   12\n",
      "WINOKUR JR. HERBERT S            12\n",
      "WODRASKA JOHN                    12\n",
      "WROBEL BRUCE                     12\n",
      "BELFER ROBERT                    11\n",
      "CHRISTODOULOU DIOMEDES           11\n",
      "DUNCAN JOHN H                    11\n",
      "FUGH JOHN L                      11\n",
      "GATHMANN WILLIAM D               11\n",
      "GILLIS JOHN                      11\n",
      "LEMAISTRE CHARLES                11\n",
      "LOWRY CHARLES P                  11\n",
      "NOLES JAMES L                    11\n",
      "BADUM JAMES P                    10\n",
      "GRAY RODNEY                      10\n",
      "JAEDICKE ROBERT                  10\n",
      "PRENTICE JAMES                   10\n",
      "WALTERS GARETH W                 10\n",
      "YEAP SOON                        10\n",
      "HIRKO JOSEPH                      9\n",
      "POWERS WILLIAM                    9\n",
      "BAZELIDES PHILIP J                8\n",
      "BERBERIAN DAVID                   8\n",
      "KISHKILL JOSEPH G                 8\n",
      "BAY FRANKLIN R                    7\n",
      "BROWN MICHAEL                     7\n",
      "HAYSLETT RODERICK J               7\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "nb_NaN_index=data.isnull().sum(axis=1)\n",
    "print(nb_NaN_index.nlargest(40))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We drop all the indices which have 12 or more features with no information (out of 15 features in total). In particular, looking at the list, we realize that there is a travel agency that does not have its place here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "irrelevant_indices=nb_NaN_index.index[nb_NaN_index>=12].values\n",
    "data.drop(irrelevant_indices,axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now start working with a cleaner dataset, and we split the 'poi' target feature appart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALLEN PHILLIP K       False\n",
      "BADUM JAMES P         False\n",
      "BANNANTINE JAMES M    False\n",
      "BAXTER JOHN C         False\n",
      "BAY FRANKLIN R        False\n",
      "Name: poi, dtype: bool\n",
      "                      salary  to_messages  total_payments  \\\n",
      "ALLEN PHILLIP K     201955.0       2902.0       4484442.0   \n",
      "BADUM JAMES P            NaN          NaN        182466.0   \n",
      "BANNANTINE JAMES M     477.0        566.0        916197.0   \n",
      "BAXTER JOHN C       267102.0          NaN       5634343.0   \n",
      "BAY FRANKLIN R      239671.0          NaN        827696.0   \n",
      "\n",
      "                    exercised_stock_options      bonus  restricted_stock  \\\n",
      "ALLEN PHILLIP K                   1729541.0  4175000.0          126027.0   \n",
      "BADUM JAMES P                      257817.0        NaN               NaN   \n",
      "BANNANTINE JAMES M                4046157.0        NaN         1757552.0   \n",
      "BAXTER JOHN C                     6680544.0  1200000.0         3942714.0   \n",
      "BAY FRANKLIN R                          NaN   400000.0          145796.0   \n",
      "\n",
      "                    shared_receipt_with_poi  total_stock_value  expenses  \\\n",
      "ALLEN PHILLIP K                      1407.0          1729541.0   13868.0   \n",
      "BADUM JAMES P                           NaN           257817.0    3486.0   \n",
      "BANNANTINE JAMES M                    465.0          5243487.0   56301.0   \n",
      "BAXTER JOHN C                           NaN         10623258.0   11200.0   \n",
      "BAY FRANKLIN R                          NaN            63014.0  129142.0   \n",
      "\n",
      "                    from_messages      other  from_this_person_to_poi  \\\n",
      "ALLEN PHILLIP K            2195.0      152.0                     65.0   \n",
      "BADUM JAMES P                 NaN        NaN                      NaN   \n",
      "BANNANTINE JAMES M           29.0   864523.0                      0.0   \n",
      "BAXTER JOHN C                 NaN  2660303.0                      NaN   \n",
      "BAY FRANKLIN R                NaN       69.0                      NaN   \n",
      "\n",
      "                    long_term_incentive  from_poi_to_this_person  \n",
      "ALLEN PHILLIP K                304805.0                     47.0  \n",
      "BADUM JAMES P                       NaN                      NaN  \n",
      "BANNANTINE JAMES M                  NaN                     39.0  \n",
      "BAXTER JOHN C                 1586055.0                      NaN  \n",
      "BAY FRANKLIN R                      NaN                      NaN  \n",
      "(128, 14)\n"
     ]
    }
   ],
   "source": [
    "labels=data['poi']\n",
    "features=data.drop('poi', axis=1)\n",
    "print(labels.head())\n",
    "print(features.head())\n",
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save this clean dataset to pickle files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels.to_pickle('labels.pkl')\n",
    "features.to_pickle('features.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start classification from cleaned data \n",
    "<a id='section3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can start to train a classifier to the new data to try to identify persons of inerest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "labels=pd.read_pickle('labels.pkl')\n",
    "features=pd.read_pickle('features.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering\n",
    "<a id='section3.1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We recall the features that we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['salary' 'to_messages' 'total_payments' 'exercised_stock_options' 'bonus'\n",
      " 'restricted_stock' 'shared_receipt_with_poi' 'total_stock_value'\n",
      " 'expenses' 'from_messages' 'other' 'from_this_person_to_poi'\n",
      " 'long_term_incentive' 'from_poi_to_this_person']\n"
     ]
    }
   ],
   "source": [
    "print(features.columns.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Some of those features do not seem to be particularly relevant for the problem at stake. For instance, the features 'to messages' and 'from message' are the total number of messages received and sent by a person. However, we can use those features to create a feature that stores the proportion of messages in relation with a POI. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['salary' 'total_payments' 'exercised_stock_options' 'bonus'\n",
      " 'restricted_stock' 'total_stock_value' 'expenses' 'other'\n",
      " 'long_term_incentive' 'poi_index']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "features['poi_index']=(features['from_this_person_to_poi']+features['from_poi_to_this_person']+\n",
    "                                  features['shared_receipt_with_poi'])/(features['to_messages']+\n",
    "                                                                        features['from_messages'])\n",
    "\n",
    "features.drop(['to_messages','from_messages','from_poi_to_this_person','from_this_person_to_poi',\n",
    "               'shared_receipt_with_poi'],axis=1,inplace=True)\n",
    "print(features.columns.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we standardize the financial features, to have all the numeric values on a similar scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      salary  total_payments  exercised_stock_options  \\\n",
      "ALLEN PHILLIP K     0.181384        0.043299                 0.050262   \n",
      "BADUM JAMES P            NaN        0.001757                 0.007411   \n",
      "BANNANTINE JAMES M  0.000000        0.008842                 0.117713   \n",
      "BAXTER JOHN C       0.240034        0.054402                 0.194417   \n",
      "BAY FRANKLIN R      0.215339        0.007988                      NaN   \n",
      "\n",
      "                       bonus  restricted_stock  total_stock_value  expenses  \\\n",
      "ALLEN PHILLIP K     0.517654          0.157232           0.036083  0.058667   \n",
      "BADUM JAMES P            NaN               NaN           0.006142  0.013189   \n",
      "BANNANTINE JAMES M       NaN          0.251180           0.107571  0.244542   \n",
      "BAXTER JOHN C       0.142497          0.377009           0.217018  0.046980   \n",
      "BAY FRANKLIN R      0.041614          0.158370           0.002179  0.563617   \n",
      "\n",
      "                       other  long_term_incentive  poi_index  \n",
      "ALLEN PHILLIP K     0.000014             0.046409   0.298018  \n",
      "BADUM JAMES P            NaN                  NaN        NaN  \n",
      "BANNANTINE JAMES M  0.083450                  NaN   0.847059  \n",
      "BAXTER JOHN C       0.256793             0.298812        NaN  \n",
      "BAY FRANKLIN R      0.000006                  NaN        NaN  \n"
     ]
    }
   ],
   "source": [
    "financial_features=['salary' ,'total_payments' ,'exercised_stock_options' ,'bonus',\n",
    " 'restricted_stock', 'total_stock_value', 'expenses' ,'other',\n",
    " 'long_term_incentive']\n",
    "for f in financial_features:\n",
    "    features[f]=(features[f]-features[f].min())/(features[f].max()-features[f].min())\n",
    "\n",
    "print(features.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are still many NaN values in this dataset, and we will replace those by 0. Indeed, for the features in consideration, in the absence of any information it makes sense to set those values to 0, instead of the mean or the median for example. Suppressing these rows would create a much smaller dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features.fillna(0.,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality reduction\n",
    "<a id='section3.2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The financial features are certainly highly correlated, we wil perform a PCA on those features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  4.70845380e-01   2.55629689e-01   9.58668143e-02   5.77620576e-02\n",
      "   3.68834174e-02   3.51819187e-02   2.49955042e-02   1.86671866e-02\n",
      "   3.75754433e-03   4.10487907e-04]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca=PCA()\n",
    "\n",
    "pca.fit(features)\n",
    "\n",
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last two features of the pca explain less than 1% of the variance. We will therefore perform dimensionality reduction and suppress those 2 features fr our analysis. The PCA step will also be needed for testing, it will be included later in a Pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(copy=True, iterated_power='auto', n_components=8, random_state=None,\n",
       "  svd_solver='auto', tol=0.0, whiten=False)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca=PCA(n_components=8)\n",
    "pca.fit(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "<a id='section4'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now try different classifyers, from very simple ones like naive bayes, to ensemble methods with gradient boosting. The assesment of these performance will be done vie 10-fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classification_report(clf, features, labels, nfolds):\n",
    "    from sklearn.model_selection import StratifiedKFold\n",
    "    from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "    skf=StratifiedKFold(n_splits=nfolds)\n",
    "    scores=[(0,0,0,0)]*nfolds\n",
    "    i=0\n",
    "    for train_index, test_index in skf.split(features, labels):\n",
    "        features_train=features.iloc[train_index]\n",
    "        features_test=features.iloc[test_index]\n",
    "        labels_train=labels[train_index]\n",
    "        labels_test=labels[test_index]\n",
    "        clf.fit(features_train,labels_train)\n",
    "        pred=clf.predict(features_test)\n",
    "        precision,recall,f_score,_ =precision_recall_fscore_support(labels_test,pred, average='binary')\n",
    "        acc=accuracy_score(labels_test,pred)\n",
    "        scores[i]=(acc,precision,recall,f_score)\n",
    "        i=i+1\n",
    "    print('Accuracy: {}   ,   Precision: {}   ,   Recall: {}   ,   F1 score: {}'.format(*np.mean(scores,axis=0)))\n",
    "    return np.mean(scores,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Naive Bayes\n",
    "<a id='section4.1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.821794871795   ,   Precision: 0.29   ,   Recall: 0.3   ,   F1 score: 0.27380952381\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.82179487,  0.29      ,  0.3       ,  0.27380952])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nfolds=10\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "GNB=GaussianNB()\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "pipe=Pipeline([('reduce_dim',PCA(n_components=8)), ('naive_bayes',GNB)])\n",
    "\n",
    "classification_report(pipe,features,labels, nfolds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without PCA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.821794871795   ,   Precision: 0.233333333333   ,   Recall: 0.2   ,   F1 score: 0.206666666667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.82179487,  0.23333333,  0.2       ,  0.20666667])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nfolds=10\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "GNB=GaussianNB()\n",
    "\n",
    "classification_report(GNB,features,labels, nfolds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression\n",
    "<a id='section4.2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With PCA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.86858974359   ,   Precision: 0.2   ,   Recall: 0.15   ,   F1 score: 0.166666666667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.86858974,  0.2       ,  0.15      ,  0.16666667])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nfolds=10\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf=LogisticRegression()\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "pipe=Pipeline([('reduce_dim',PCA(n_components=8)), ('Log_reg',clf)])\n",
    "\n",
    "classification_report(pipe,features,labels, nfolds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without PCA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.867948717949   ,   Precision: 0.1   ,   Recall: 0.05   ,   F1 score: 0.0666666666667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.86794872,  0.1       ,  0.05      ,  0.06666667])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nfolds=10\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf=LogisticRegression()\n",
    "\n",
    "classification_report(clf,features,labels, nfolds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to improve a little bit this simple classifier using some regularization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.876282051282   ,   Precision: 0.2   ,   Recall: 0.2   ,   F1 score: 0.2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.87628205,  0.2       ,  0.2       ,  0.2       ])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nfolds=10\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf=LogisticRegression()\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "pipe=Pipeline([('reduce_dim',PCA(n_components=8)), ('Log_reg',clf)])\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "C = np.logspace(-4, 4, 50)\n",
    "penalty = ['l1', 'l2']\n",
    "\n",
    "parameters = dict(Log_reg__C=C,\n",
    "                  Log_reg__penalty=penalty)\n",
    "\n",
    "clf = GridSearchCV(pipe, parameters,n_jobs=-1, cv=10)\n",
    "\n",
    "\n",
    "clf.fit(features, labels)\n",
    "\n",
    "best_clf=clf.best_estimator_\n",
    "\n",
    "classification_report(best_clf,features,labels, nfolds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These scores are not very good, we shall try more sophisticated methods. Let's first try a simple Decision tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree\n",
    "<a id='section4.3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With PCA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/thumeau/anaconda/envs/py2/lib/python2.7/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/thumeau/anaconda/envs/py2/lib/python2.7/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/thumeau/anaconda/envs/py2/lib/python2.7/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/thumeau/anaconda/envs/py2/lib/python2.7/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.598076923077   ,   Precision: 0.240952380952   ,   Recall: 0.6   ,   F1 score: 0.329365079365\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.59807692,  0.24095238,  0.6       ,  0.32936508])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import f1_score, precision_score\n",
    "from sklearn.metrics import make_scorer\n",
    "clf=DecisionTreeClassifier()\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "pipe=Pipeline([('reduce_dim',PCA(n_components=8)), ('decision_tree',clf)])\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameters = dict(decision_tree__criterion=['gini', 'entropy'],\n",
    "                  decision_tree__max_features=[5,6,7,8], decision_tree__max_depth=[None,3,5,10], \n",
    "                 decision_tree__min_samples_split =[2,5], decision_tree__class_weight=[None, 'balanced'])\n",
    "\n",
    "clf = GridSearchCV(pipe, parameters,n_jobs=-1, cv=10, scoring=make_scorer(f1_score))\n",
    "\n",
    "clf.fit(features, labels)\n",
    "\n",
    "best_clf=clf.best_estimator_\n",
    "\n",
    "classification_report(best_clf,features,labels, nfolds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good recall but still poor precision, therefore we try some ensemble method:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "<a id='section4.4'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/thumeau/anaconda/envs/py2/lib/python2.7/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/thumeau/anaconda/envs/py2/lib/python2.7/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/thumeau/anaconda/envs/py2/lib/python2.7/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/thumeau/anaconda/envs/py2/lib/python2.7/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.75   ,   Precision: 0.15   ,   Recall: 0.3   ,   F1 score: 0.188333333333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.75      ,  0.15      ,  0.3       ,  0.18833333])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, precision_score\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "pipe=Pipeline([('reduce_dim',PCA(n_components=8)), ('r_forest',RandomForestClassifier())])\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameters = dict(r_forest__criterion=['gini', 'entropy'], r_forest__n_estimators=[5,10],\n",
    "                  r_forest__max_features=[5,6,7,8], r_forest__max_depth=[None,3,5,10], \n",
    "                 r_forest__min_samples_split =[2,5], r_forest__class_weight=[None, 'balanced'], r_forest__n_jobs=[-1])\n",
    "\n",
    "clf = GridSearchCV(pipe, parameters,n_jobs=-1, cv=10, scoring=make_scorer(precision_score))\n",
    "\n",
    "clf.fit(features, labels)\n",
    "\n",
    "best_clf=clf.best_estimator_\n",
    "\n",
    "classification_report(best_clf,features,labels, nfolds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost\n",
    "<a id='section4.5'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/thumeau/anaconda/envs/py2/lib/python2.7/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/thumeau/anaconda/envs/py2/lib/python2.7/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/thumeau/anaconda/envs/py2/lib/python2.7/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/thumeau/anaconda/envs/py2/lib/python2.7/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.860256410256   ,   Precision: 0.316666666667   ,   Recall: 0.35   ,   F1 score: 0.33\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.86025641,  0.31666667,  0.35      ,  0.33      ])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nfolds=10\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "ada=AdaBoostClassifier()\n",
    "\n",
    "lr=[0.01, 0.05, 0.1, 0.5, 1., 2., 5., 10.] \n",
    "\n",
    "parameters=dict(adaboost__learning_rate=lr, adaboost__n_estimators=[10,50,100])\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "pipe=Pipeline([('adaboost',ada)])\n",
    "\n",
    "clf = GridSearchCV(pipe, parameters,n_jobs=-1, cv=5, scoring=make_scorer(f1_score))\n",
    "\n",
    "clf.fit(features, labels)\n",
    "\n",
    "best_clf=clf.best_estimator_\n",
    "\n",
    "classification_report(best_clf,features,labels, nfolds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

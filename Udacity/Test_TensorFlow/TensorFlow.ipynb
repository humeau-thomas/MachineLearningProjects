{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "x=tf.Variable(3,name='x')\n",
    "y=tf.Variable(4, name='y')\n",
    "f=x*x*y+y+2\n",
    "init=tf.global_variables_initializer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "sess=tf.Session()\n",
    "sess.run(x.initializer)\n",
    "sess.run(y.initializer)\n",
    "result=sess.run(f)\n",
    "print(result)\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    x.initializer.run()\n",
    "    y.initializer.run()\n",
    "    result=f.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    result=f.eval()\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression on California housing dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal equations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -3.74651413e+01]\n",
      " [  4.35734153e-01]\n",
      " [  9.33829229e-03]\n",
      " [ -1.06622010e-01]\n",
      " [  6.44106984e-01]\n",
      " [ -4.25131839e-06]\n",
      " [ -3.77322501e-03]\n",
      " [ -4.26648885e-01]\n",
      " [ -4.40514028e-01]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "housing=fetch_california_housing()\n",
    "m,n=housing.data.shape\n",
    "\n",
    "housing_data_plus_bias=np.c_[np.ones((m,1)), housing.data]\n",
    "\n",
    "X=tf.constant(housing_data_plus_bias, dtype=tf.float32, name='X')\n",
    "y=tf.constant(housing.target.reshape(-1,1), dtype=tf.float32, name='y')\n",
    "XT=tf.transpose(X)\n",
    "\n",
    "theta=tf.matmul(tf.matmul(tf.matrix_inverse(tf.matmul(XT,X)),XT),y)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    theta_value=theta.eval()\n",
    "\n",
    "print(theta_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to scale the data first for better convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaled_housing_data=scaler.fit_transform(housing.data)\n",
    "\n",
    "scaled_housing_data_plus_bias=np.c_[np.ones((m,1)), scaled_housing_data]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loop 0 MSE= 4.85334\n",
      "loop 100 MSE= 0.747335\n",
      "loop 200 MSE= 0.668665\n",
      "loop 300 MSE= 0.639556\n",
      "loop 400 MSE= 0.617165\n",
      "loop 500 MSE= 0.599403\n",
      "loop 600 MSE= 0.585237\n",
      "loop 700 MSE= 0.573893\n",
      "loop 800 MSE= 0.564771\n",
      "loop 900 MSE= 0.557409\n",
      "[[ 2.06855249]\n",
      " [ 0.97460502]\n",
      " [ 0.16666   ]\n",
      " [-0.50069165]\n",
      " [ 0.48311004]\n",
      " [ 0.01104329]\n",
      " [-0.04647031]\n",
      " [-0.4144997 ]\n",
      " [-0.40015447]]\n"
     ]
    }
   ],
   "source": [
    "nloops=1000\n",
    "learning_rate=0.01\n",
    "\n",
    "X=tf.constant(scaled_housing_data_plus_bias,dtype=tf.float32, name='X')\n",
    "y=tf.constant(housing.target.reshape(-1,1),dtype=tf.float32, name='y')\n",
    "theta=tf.Variable(tf.random_uniform([n+1,1],-1.,1.), name='theta')\n",
    "y_pred=tf.matmul(X,theta, name='prediction')\n",
    "error=y_pred-y\n",
    "mse=tf.reduce_mean(tf.square(error, name='mse'))\n",
    "\n",
    "gradients=2/m*tf.matmul(tf.transpose(X),error)\n",
    "\n",
    "training_op=tf.assign(theta,theta-learning_rate*gradients)\n",
    "\n",
    "init=tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for loop in range(nloops):\n",
    "        if loop%100==0:\n",
    "            print('loop', loop, 'MSE=', mse.eval())\n",
    "        sess.run(training_op)\n",
    "        \n",
    "    best_theta=theta.eval()\n",
    "    \n",
    "    print(best_theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using autodiff:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loop 0 MSE= 10.9403\n",
      "loop 100 MSE= 0.73624\n",
      "loop 200 MSE= 0.533643\n",
      "loop 300 MSE= 0.526731\n",
      "loop 400 MSE= 0.525957\n",
      "loop 500 MSE= 0.525583\n",
      "loop 600 MSE= 0.52531\n",
      "loop 700 MSE= 0.525099\n",
      "loop 800 MSE= 0.524938\n",
      "loop 900 MSE= 0.524811\n",
      "[[ 2.06855226]\n",
      " [ 0.84255821]\n",
      " [ 0.12524015]\n",
      " [-0.28237295]\n",
      " [ 0.31628352]\n",
      " [-0.00231135]\n",
      " [-0.04015082]\n",
      " [-0.84042633]\n",
      " [-0.81221974]]\n"
     ]
    }
   ],
   "source": [
    "nloops=1000\n",
    "learning_rate=0.01\n",
    "\n",
    "X=tf.constant(scaled_housing_data_plus_bias,dtype=tf.float32, name='X')\n",
    "y=tf.constant(housing.target.reshape(-1,1),dtype=tf.float32, name='y')\n",
    "theta=tf.Variable(tf.random_uniform([n+1,1],-1.,1.), name='theta')\n",
    "y_pred=tf.matmul(X,theta, name='prediction')\n",
    "error=y_pred-y\n",
    "mse=tf.reduce_mean(tf.square(error, name='mse'))\n",
    "\n",
    "#difference is here\n",
    "gradients=tf.gradients(mse,[theta])[0]\n",
    "\n",
    "training_op=tf.assign(theta,theta-learning_rate*gradients)\n",
    "\n",
    "init=tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for loop in range(nloops):\n",
    "        if loop%100==0:\n",
    "            print('loop', loop, 'MSE=', mse.eval())\n",
    "        sess.run(training_op)\n",
    "        \n",
    "    best_theta=theta.eval()\n",
    "    \n",
    "    print(best_theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using an optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loop 0 MSE= 3.26923\n",
      "loop 500 MSE= 0.561195\n",
      "loop 1000 MSE= 0.537394\n",
      "loop 1500 MSE= 0.529233\n",
      "loop 2000 MSE= 0.526227\n",
      "loop 2500 MSE= 0.525074\n",
      "loop 3000 MSE= 0.524621\n",
      "loop 3500 MSE= 0.524441\n",
      "loop 4000 MSE= 0.524369\n",
      "loop 4500 MSE= 0.52434\n",
      "[[ 2.06855249]\n",
      " [ 0.83274233]\n",
      " [ 0.11935294]\n",
      " [-0.27139792]\n",
      " [ 0.31053811]\n",
      " [-0.0043248 ]\n",
      " [-0.03944373]\n",
      " [-0.8925699 ]\n",
      " [-0.86358738]]\n"
     ]
    }
   ],
   "source": [
    "nloops=5000\n",
    "learning_rate=0.01\n",
    "\n",
    "X=tf.constant(scaled_housing_data_plus_bias,dtype=tf.float32, name='X')\n",
    "y=tf.constant(housing.target.reshape(-1,1),dtype=tf.float32, name='y')\n",
    "theta=tf.Variable(tf.random_uniform([n+1,1],-1.,1.), name='theta')\n",
    "y_pred=tf.matmul(X,theta, name='prediction')\n",
    "error=y_pred-y\n",
    "mse=tf.reduce_mean(tf.square(error, name='mse'))\n",
    "\n",
    "#use an optimizer\n",
    "optimizer=tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "training_op=optimizer.minimize(mse)\n",
    "\n",
    "init=tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for loop in range(nloops):\n",
    "        if loop%500==0:\n",
    "            print('loop', loop, 'MSE=', mse.eval())\n",
    "        sess.run(training_op)\n",
    "        \n",
    "    best_theta=theta.eval()\n",
    "    \n",
    "    print(best_theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Mini-batch Gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 6.  7.  8.]]\n",
      "[[  9.  10.  11.]\n",
      " [ 12.  13.  14.]]\n"
     ]
    }
   ],
   "source": [
    "A=tf.placeholder(tf.float32, shape=(None,3))\n",
    "B=A+5\n",
    "with tf.Session() as sess:\n",
    "    B_val_1=B.eval(feed_dict={A: [[1,2,3]]})\n",
    "    B_val_2=B.eval(feed_dict={A: [[4,5,6], [7,8,9]]})\n",
    "\n",
    "    \n",
    "print(B_val_1)\n",
    "print(B_val_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loop 0 MSE= 0.133957\n",
      "loop 5 MSE= 0.125182\n",
      "[[ 2.00519872]\n",
      " [ 0.79733837]\n",
      " [ 0.13927931]\n",
      " [-0.25245261]\n",
      " [ 0.2886897 ]\n",
      " [-0.00551016]\n",
      " [-0.01423678]\n",
      " [-0.83756173]\n",
      " [-0.86223739]]\n"
     ]
    }
   ],
   "source": [
    "nloops=10\n",
    "learning_rate=0.01\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaled_housing_data=scaler.fit_transform(housing.data)\n",
    "\n",
    "scaled_housing_data_plus_bias=np.c_[np.ones((m,1)), scaled_housing_data]\n",
    "\n",
    "import pandas as pd\n",
    "df=pd.DataFrame(np.c_[housing.target, scaled_housing_data], columns=['y']+housing.feature_names)\n",
    "df['Bias']=np.ones((m,1))\n",
    "\n",
    "cols=list(df.columns.values)\n",
    "cols_temp=cols[1:len(cols)-1]\n",
    "\n",
    "cols[1]='Bias'\n",
    "cols[2:]=cols_temp\n",
    "\n",
    "df=df[cols]\n",
    "\n",
    "\n",
    "df.to_csv('data.csv',)\n",
    "\n",
    "batch_size=100\n",
    "n_batches=int(np.ceil(m/batch_size))\n",
    "\n",
    "#Placeholder to feed the minibatches into X and y\n",
    "X=tf.placeholder(tf.float32, shape=(None, n+1), name='X')\n",
    "y=tf.placeholder(tf.float32, shape=(None, 1), name='y')\n",
    "\n",
    "theta=tf.Variable(tf.random_uniform([n+1,1],-1.,1.), name='theta')\n",
    "y_pred=tf.matmul(X,theta, name='prediction')\n",
    "error=y_pred-y\n",
    "mse=tf.reduce_mean(tf.square(error, name='mse'))\n",
    "\n",
    "#use an optimizer\n",
    "optimizer=tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "training_op=optimizer.minimize(mse)\n",
    "\n",
    "init=tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for loop in range(nloops):\n",
    "        for batch_index in range(n_batches):\n",
    "            #get the corresponding minibatch\n",
    "            data_batch=df.iloc[batch_index*batch_size: min((batch_index+1)*batch_size, m)]\n",
    "            y_batch=np.asarray(data_batch.y).reshape((-1,1))\n",
    "            X_batch=np.asarray(data_batch[data_batch.columns[1:]])\n",
    "            #and feed it to the training operation\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        if loop%5==0:\n",
    "            #the eval of mse requires X and y\n",
    "            print('loop', loop, 'MSE=', mse.eval(feed_dict={X: X_batch, y: y_batch }))\n",
    "    \n",
    "    best_theta=theta.eval()\n",
    "    \n",
    "    print(best_theta)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Saving the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loop 0 MSE= 0.203762\n",
      "loop 5 MSE= 0.124818\n",
      "loop 10 MSE= 0.123936\n",
      "loop 15 MSE= 0.124325\n",
      "loop 20 MSE= 0.124396\n",
      "loop 25 MSE= 0.124398\n",
      "loop 30 MSE= 0.124393\n",
      "loop 35 MSE= 0.12439\n",
      "loop 40 MSE= 0.124389\n",
      "loop 45 MSE= 0.124388\n",
      "[[ 2.00290966]\n",
      " [ 0.78347826]\n",
      " [ 0.13612111]\n",
      " [-0.23014921]\n",
      " [ 0.27157745]\n",
      " [-0.00706323]\n",
      " [-0.01321218]\n",
      " [-0.86899805]\n",
      " [-0.89598686]]\n"
     ]
    }
   ],
   "source": [
    "#Parameters for the gradient descent\n",
    "nloops=50\n",
    "learning_rate=0.01\n",
    "\n",
    "#Preprocessing of the data to improve convergence of grandient descent\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaled_housing_data=scaler.fit_transform(housing.data)\n",
    "\n",
    "#add a bias unit\n",
    "scaled_housing_data_plus_bias=np.c_[np.ones((m,1)), scaled_housing_data]\n",
    "\n",
    "#tranform the data into a pandas dataframe to save it for later, and reorganize the columns\n",
    "import pandas as pd\n",
    "df=pd.DataFrame(np.c_[housing.target, scaled_housing_data], columns=['y']+housing.feature_names)\n",
    "df['Bias']=np.ones((m,1))\n",
    "\n",
    "cols=list(df.columns.values)\n",
    "cols_temp=cols[1:len(cols)-1]\n",
    "\n",
    "cols[1]='Bias'\n",
    "cols[2:]=cols_temp\n",
    "\n",
    "df=df[cols]\n",
    "\n",
    "df.to_csv('data.csv',)\n",
    "\n",
    "\n",
    "#parameter for the minibatch gradient descent\n",
    "batch_size=100\n",
    "n_batches=int(np.ceil(m/batch_size))\n",
    "\n",
    "\n",
    "#define first the different nodes of the graph\n",
    "#Placeholder to feed the minibatches into X and y\n",
    "X=tf.placeholder(tf.float32, shape=(None, n+1), name='X')\n",
    "y=tf.placeholder(tf.float32, shape=(None, 1), name='y')\n",
    "\n",
    "#the variable to train\n",
    "theta=tf.Variable(tf.random_uniform([n+1,1],-1.,1.), name='theta')\n",
    "\n",
    "#the prediction\n",
    "y_pred=tf.matmul(X,theta, name='prediction')\n",
    "\n",
    "#the error and the function to minimize\n",
    "error=y_pred-y\n",
    "mse=tf.reduce_mean(tf.square(error, name='mse'))\n",
    "\n",
    "#use an optimizer\n",
    "optimizer=tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "training_op=optimizer.minimize(mse)\n",
    "\n",
    "#initializer\n",
    "init=tf.global_variables_initializer()\n",
    "\n",
    "#Saver\n",
    "saver=tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for loop in range(nloops):\n",
    "        if loop%5 ==0: \n",
    "            save_path=saver.save(sess,'tmp/my_model.ckpt')\n",
    "        for batch_index in range(n_batches):\n",
    "            #get the corresponding minibatch\n",
    "            data_batch=df.iloc[batch_index*batch_size: min((batch_index+1)*batch_size, m)]\n",
    "            y_batch=np.asarray(data_batch.y).reshape((-1,1))\n",
    "            X_batch=np.asarray(data_batch[data_batch.columns[1:]])\n",
    "            #and feed it to the training operation\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        if loop%5==0:\n",
    "            #the eval of mse requires X and y\n",
    "            print('loop', loop, 'MSE=', mse.eval(feed_dict={X: X_batch, y: y_batch }))\n",
    "    \n",
    "    best_theta=theta.eval()\n",
    "    print(best_theta)\n",
    "    save_path=saver.save(sess,'tmp/my_model_final.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importation of a model from a checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from tmp/my_model_final.ckpt\n",
      "loop 0 MSE= 0.124388\n",
      "loop 5 MSE= 0.124388\n",
      "loop 10 MSE= 0.124388\n",
      "loop 15 MSE= 0.124388\n",
      "loop 20 MSE= 0.124388\n",
      "loop 25 MSE= 0.124388\n",
      "loop 30 MSE= 0.124388\n",
      "loop 35 MSE= 0.124388\n",
      "loop 40 MSE= 0.124388\n",
      "loop 45 MSE= 0.124388\n",
      "[[ 2.00290895]\n",
      " [ 0.78347152]\n",
      " [ 0.13612019]\n",
      " [-0.23013733]\n",
      " [ 0.27156752]\n",
      " [-0.00706378]\n",
      " [-0.01321162]\n",
      " [-0.86900991]\n",
      " [-0.89599925]]\n"
     ]
    }
   ],
   "source": [
    "saver=tf.train.import_meta_graph('tmp/my_model_final.ckpt.meta')\n",
    "    \n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess,'tmp/my_model_final.ckpt')\n",
    "    for loop in range(nloops):\n",
    "        for batch_index in range(n_batches):\n",
    "            #get the corresponding minibatch\n",
    "            data_batch=df.iloc[batch_index*batch_size: min((batch_index+1)*batch_size, m)]\n",
    "            y_batch=np.asarray(data_batch.y).reshape((-1,1))\n",
    "            X_batch=np.asarray(data_batch[data_batch.columns[1:]])\n",
    "            #and feed it to the training operation\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        if loop%5==0:\n",
    "            #the eval of mse requires X and y\n",
    "            print('loop', loop, 'MSE=', mse.eval(feed_dict={X: X_batch, y: y_batch }))\n",
    "    \n",
    "    best_theta=theta.eval()\n",
    "    print(best_theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.07013464]\n",
      " [ 0.83224338]\n",
      " [ 0.11718354]\n",
      " [-0.25360975]\n",
      " [ 0.33584762]\n",
      " [ 0.00311589]\n",
      " [-0.01102609]\n",
      " [-0.90628386]\n",
      " [-0.87120152]]\n"
     ]
    }
   ],
   "source": [
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "reset_graph()\n",
    "df=pd.read_csv('data.csv', index_col=0, header=0)\n",
    "df_X=df.drop('y', axis=1)\n",
    "df_y=df.y\n",
    "\n",
    "\n",
    "def fetch_batch(loop, batch_index, batch_size):\n",
    "    np.random.seed(loop * n_batches + batch_index)  \n",
    "    indices = np.random.randint(m, size=batch_size)  \n",
    "    X_batch = np.asarray(df_X.iloc[indices])\n",
    "    y_batch = np.asarray(df_y.iloc[indices]).reshape((-1,1))\n",
    "    return X_batch, y_batch\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "now=datetime.utcnow().strftime('%Y%m%d%H%M%S')\n",
    "root_logdir='tf_logs'\n",
    "logdir='{}/run-{}/'.format(root_logdir, now)\n",
    "\n",
    "\n",
    "#parameter for the minibatch gradient descent\n",
    "batch_size=100\n",
    "n_batches=int(np.ceil(m/batch_size))\n",
    "\n",
    "#Parameters for the gradient descent\n",
    "nloops=10\n",
    "learning_rate=0.01\n",
    "\n",
    "\n",
    "#define first the different nodes of the graph\n",
    "#Placeholder to feed the minibatches into X and y\n",
    "X=tf.placeholder(tf.float32, shape=(None, n+1), name='X')\n",
    "y=tf.placeholder(tf.float32, shape=(None, 1), name='y')\n",
    "\n",
    "#the variable to train\n",
    "theta=tf.Variable(tf.random_uniform([n+1,1],-1.,1.), name='theta')\n",
    "\n",
    "#the prediction\n",
    "y_pred=tf.matmul(X,theta, name='prediction')\n",
    "\n",
    "#the error and the function to minimize\n",
    "error=y_pred-y\n",
    "mse=tf.reduce_mean(tf.square(error, name='mse'))\n",
    "\n",
    "#use an optimizer\n",
    "optimizer=tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "training_op=optimizer.minimize(mse)\n",
    "\n",
    "#initializer\n",
    "init=tf.global_variables_initializer()\n",
    "\n",
    "mse_summary=tf.summary.scalar('MSE', mse)\n",
    "file_writer=tf.summary.FileWriter(logdir, tf.get_default_graph())\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for loop in range(nloops):\n",
    "        for batch_index in range(n_batches):\n",
    "            X_batch, y_batch=fetch_batch(loop, batch_index, batch_size)\n",
    "            if batch_index % 10==0:\n",
    "                summary_str=mse_summary.eval(feed_dict={X:X_batch, y:y_batch})\n",
    "                step=loop*n_batches+batch_index\n",
    "                file_writer.add_summary(summary_str,step)\n",
    "            #get the corresponding minibatch\n",
    "            #and feed it to the training operation\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "    best_theta = theta.eval()\n",
    "file_writer.close()\n",
    "\n",
    "print(best_theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Name Scopes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.07013464]\n",
      " [ 0.83224338]\n",
      " [ 0.11718354]\n",
      " [-0.25360975]\n",
      " [ 0.33584762]\n",
      " [ 0.00311589]\n",
      " [-0.01102609]\n",
      " [-0.90628386]\n",
      " [-0.87120152]]\n"
     ]
    }
   ],
   "source": [
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "reset_graph()\n",
    "df=pd.read_csv('data.csv', index_col=0, header=0)\n",
    "df_X=df.drop('y', axis=1)\n",
    "df_y=df.y\n",
    "\n",
    "\n",
    "def fetch_batch(loop, batch_index, batch_size):\n",
    "    np.random.seed(loop * n_batches + batch_index)  \n",
    "    indices = np.random.randint(m, size=batch_size)  \n",
    "    X_batch = np.asarray(df_X.iloc[indices])\n",
    "    y_batch = np.asarray(df_y.iloc[indices]).reshape((-1,1))\n",
    "    return X_batch, y_batch\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "now=datetime.utcnow().strftime('%Y%m%d%H%M%S')\n",
    "root_logdir='tf_logs'\n",
    "logdir='{}/run-{}/'.format(root_logdir, now)\n",
    "\n",
    "\n",
    "#parameter for the minibatch gradient descent\n",
    "batch_size=100\n",
    "n_batches=int(np.ceil(m/batch_size))\n",
    "\n",
    "#Parameters for the gradient descent\n",
    "nloops=10\n",
    "learning_rate=0.01\n",
    "\n",
    "\n",
    "#define first the different nodes of the graph\n",
    "#Placeholder to feed the minibatches into X and y\n",
    "X=tf.placeholder(tf.float32, shape=(None, n+1), name='X')\n",
    "y=tf.placeholder(tf.float32, shape=(None, 1), name='y')\n",
    "\n",
    "#the variable to train\n",
    "theta=tf.Variable(tf.random_uniform([n+1,1],-1.,1.), name='theta')\n",
    "\n",
    "#the prediction\n",
    "y_pred=tf.matmul(X,theta, name='prediction')\n",
    "\n",
    "#the error and the function to minimize\n",
    "with tf.name_scope('loss') as scope:\n",
    "    error=y_pred-y\n",
    "    mse=tf.reduce_mean(tf.square(error, name='mse'))\n",
    "\n",
    "#use an optimizer\n",
    "optimizer=tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "training_op=optimizer.minimize(mse)\n",
    "\n",
    "#initializer\n",
    "init=tf.global_variables_initializer()\n",
    "\n",
    "mse_summary=tf.summary.scalar('MSE', mse)\n",
    "file_writer=tf.summary.FileWriter(logdir, tf.get_default_graph())\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for loop in range(nloops):\n",
    "        for batch_index in range(n_batches):\n",
    "            X_batch, y_batch=fetch_batch(loop, batch_index, batch_size)\n",
    "            if batch_index % 10==0:\n",
    "                summary_str=mse_summary.eval(feed_dict={X:X_batch, y:y_batch})\n",
    "                step=loop*n_batches+batch_index\n",
    "                file_writer.add_summary(summary_str,step)\n",
    "            #get the corresponding minibatch\n",
    "            #and feed it to the training operation\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "    best_theta = theta.eval()\n",
    "file_writer.close()\n",
    "\n",
    "print(best_theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(X):\n",
    "    with tf.name_scope('relu'): \n",
    "        w_shape=(int(X.get_shape()[1]),1)\n",
    "        w=tf.Variable(tf.random_normal(w_shape), name='weights')\n",
    "        b=tf.Variable(0.0, name='bias')\n",
    "        z=tf.add(tf.matmul(X,w),b, name='z')\n",
    "        return tf.maximum(z, 0., name='relu')\n",
    "\n",
    "reset_graph()\n",
    "\n",
    "n_features=3\n",
    "X=tf.placeholder(dtype=tf.float32, shape=(None,n_features), name='X')\n",
    "relus=[relu(X) for i in range(5)]\n",
    "output=tf.add_n(relus, name='output')\n",
    "\n",
    "file_writer = tf.summary.FileWriter(\"logs/relu2\", tf.get_default_graph())\n",
    "file_writer.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
